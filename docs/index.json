[{"authors":null,"categories":null,"content":"I am a PhD candidate at School of Informatics, The University of Edinburgh and member of the Institute for Language, Cognition and Computation (ILCC). My interests lie in healthy aging and discovering early cognitive decline or impairment. I am focusing on neurolinguistic verbal/utterance data to explore semantic analysis using Natural Language Processing methods. My main goal is investigating important features on Sequence analysis in the light of various of feature extraction methodologies. Developing Natural Language Processing based applications for healthcare professionals applying automated features derived from cognitive tests, particularly verbal fluency.\nMy research interests are;\nSemantic analysis Machine learning Natural Language Processing and applications Statistics Cognitive Psychology Cognitive Assessment Tests Verbal Fluency ","date":1660521600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1660521600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a PhD candidate at School of Informatics, The University of Edinburgh and member of the Institute for Language, Cognition and Computation (ILCC). My interests lie in healthy aging and discovering early cognitive decline or impairment.","tags":null,"title":"Rabia YAŞA KOŞTAŞ","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Kahraman Kostas","Rabia YAŞA KOŞTAŞ","Francisco Zampella","Firas Alsehly"],"categories":null,"content":"","date":1660521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660521600,"objectID":"fa56aec27f0348d0b9027c58fa4d403b","permalink":"https://example.com/publication/iotdevid1/","publishdate":"2022-08-15T00:00:00Z","relpermalink":"/publication/iotdevid1/","section":"publication","summary":"In recent years WiFi became the primary source of information to locate a person or device indoor. Collecting RSSI values as reference measurements with known positions, known as WiFi fingerprinting, is commonly used in various positioning methods and algorithms that appear in literature. However, measuring the spatial distance between given set of WiFi fingerprints is heavily affected by the selection of the signal distance function used to model signal space as geospatial distance. In this study, the authors proposed utilization of machine learning to improve the estimation of geospatial distance between fingerprints. This research examined data collected from 13 different open datasets to provide a broad representation aiming for general model that can be used in any indoor environment. The proposed novel approach extracted data features by examining a set of commonly used signal distance metrics via feature selection process that includes feature analysis and genetic algorithm. To demonstrate that the output of this research is venue independent, all models were tested on datasets previously excluded during the training and validation phase. Finally, various machine learning algorithms were compared using wide variety of evaluation metrics including ability to scale out the test bed to real world unsolicited datasets.","tags":[],"title":"WiFi Based Distance Estimation Using Supervised Machine Learning","type":"publication"},{"authors":null,"categories":null,"content":"IoTDevID: A Behavior-Based Device Identification Method for the IoT Overview In this repository you will find a Python implementation of the methods in the paper IoTDevID: A Behavior-Based Device Identification Method for the IoT.\nKahraman Kostas, Mike Just, and Michael A. Lones. IoTDevID: A Behavior-Based Device Identification Method for the IoT, arXiv preprint, arxiv:2102.08866v2, 2021.\nWhat is IoTDevID? Device identification is one way to secure a network of IoT devices, whereby devices identified as suspicious can subsequently be isolated from a network. In this study, we present a machine learning-based method, IoTDevID, that recognises devices through characteristics of their network packets. As a result of using a rigorous feature analysis and selection process, our study offers a generalizable and realistic approach to modelling device behavior, achieving high predictive accuracy across two public datasets. The model’s underlying feature set is shown to be more predictive than existing feature sets used for device identification, and is shown to generalise to data unseen during the feature selection process. Unlike most existing approaches to IoT device identification, IoTDevID is able to detect devices using non-IP and low-energy protocols.\nRequirements and Infrastructure: Wireshark and Python 3.6 were used to create the application files. Before running the files, it must be ensured that Wireshark, Python 3.6+ and the following libraries are installed.\nLibrary Task Scapy Packet(Pcap) crafting tshark Packet(Pcap) crafting Sklearn Machine Learning \u0026amp; Data Preparation xverse Feature importance/voting Numpy Mathematical Operations Pandas Data Analysis Matplotlib Graphics and Visuality Seaborn Graphics and Visuality graphviz Graphics and Visuality The technical specifications of the computer used for experiments are given below.\nCentral Processing Unit : Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz 2.90 GHz Random Access Memory : 8 GB (7.74 GB usable) Operating System : Windows 10 Pro 64-bit Graphics Processing Unit : AMD Readon (TM) 530 Implementation: The implementation phase consists of 5 steps, which are:\nFeature Extraction Feature Selection Algorithm Selection Performance Evaluation Comparison with Previous Work Each of these steps is implemented using one or more Python files. The same file was saved with both “py” and “ipynb” extensions. The code they contain is exactly the same. The file with the ipynb extension has the advantage of saving the state of the last run of that file and the screen output. Thus, screen output can be seen without re-running the files. Files with the ipynb extension can be run using jupyter notebook.\n01 Feature Extraction (PCAP2CSV) Section III.C in the article There are four files relevant to this section:\n01.1 Aalto feature extraction IoTDevID 01.2 Aalto feature extraction IoTSense - IoT Sentinel 01.3 UNSW feature extraction IoTDevID 01.4 UNSW feature extraction IoTSense - IoT Sentinel These files convert the files with pcap extension to single packet-based, CSV extension fingerprint files (IoT Sentinel, IoTSense, IoTDevID individual packet based feature sets) and creates the labeling.\nThe processed datasets are shared in the repository. However, raw versions of the datasets used in the study and their addresses are given below.\nDataset capture year Number of Devices Type Aalto University 2016 31 Benign UNSW-Sydney IEEE TMC 2016 31 Benign UNSW-Sydney ACM SOSR 2018 28 Benign \u0026amp; Malicious Since the UNSW data are very large, we filter the data on a device and session basis. You can access the Pcap files obtained from this filtering process from this link (Used Pcap Files).\nIn addition, the CSVs.zip file contains the feature sets that are the output of this step and that we used in our experiments. These files:\nAalto_test_IoTDevID.csv Aalto_train_IoTDevID.csv Aalto_IoTSense_Test.csv Aalto_IoTSense_Train.csv Aalto_IoTSentinel_Test.csv Aalto_IoTSentinel_Train.csv UNSW_test_IoTDevID.csv UNSW_train_IoTDevID.csv UNSW_IoTSense_Test.csv UNSW_IoTSense_Train.csv UNSW_IoTSentinel_Test.csv UNSW_IoTSentinel_Train.csv 02 Feature Selection Section IV.A in the article There are three files relevant to this section.\n02.1 Feature importance voting and pre-assessment of features: This file calculates the importance scores for each feature using six feature score calculation methods. It then votes for features using these scores. It lists the feature scores and the votes they have received and shows them on a plot. The six feature importance score calculation methods used are as follows.\nInformation Value using Weight of evidence. Variable Importance using Random Forest. Recursive Feature Elimination. Variable Importance using Extra trees classifier. Chi-Square best variables. L1-based feature selection. 02.2 Comparison of isolated data and CV methods: In this file, the results of the isolated test-training data and the cross-validated data are compared.\n02.3 Feature selection process using …","date":1653609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653609600,"objectID":"f019b13f7ec674a0f61061ec9be7510d","permalink":"https://example.com/project/iotdevidv2-main/readme/","publishdate":"2022-05-27T00:00:00Z","relpermalink":"/project/iotdevidv2-main/readme/","section":"project","summary":"In this repository you will find a Python implementation of the methods in the paper IoTDevID - A Behavior-Based Device Identification Method for the IoT.","tags":["Device Identification","Machine Learning"],"title":"IoTDevID - A Behavior-Based Device Identification Method for the IoT","type":"project"},{"authors":null,"categories":null,"content":"Prediction of wind turbine power generation from real-time SCADA data The models to be created for this problem are the models that will predict the power values ​​expected to be produced by a wind turbine. Thus, by comparing the actual production of a wind turbine with these estimation results, it will be presented to the investor to what extent the turbine produces less than it should be. From this point of view, the investor will be able to realize that there is a performance problem related to the turbine and will be able to initiate root cause analysis.\nThe data set presented in the problem consists of real-time SCADA data. Each data value belongs only to the relevant time period and the input variables transmitted in the data set for the time period to be predicted are prepared to be used to predict the power generation result in the same time period.\nIn the shared data set, the real-time power generation amount (Power(kW)) of a wind turbine belonging to Enerjisa Üretim between 01.01.2019 and 14.08.2021 is given on a 10-minute basis.\nInformation presented in the dataset and its units Column Unit Timestamp () Gearbox_T1_High_Speed_Shaft_Temperature (°C) Gearbox_T3_High_Speed_Shaft_Temperature (°C) Gearbox_T1_Intermediate_Speed_Shaft_Temperature (°C) Temperature Gearbox Bearing Hollow Shaft (°C) Tower Acceleration Normal (mm/s²) Gearbox_Oil-2_Temperature (°C) Tower Acceleration Lateral (mm/s²) Temperature Bearing_A (°C) Temperature Trafo-3 (°C) Gearbox_T3_Intermediate_Speed_Shaft_Temperature (°C) Gearbox_Oil-1_Temperature (°C) Gearbox_Oil_Temperature (°C) Torque (%) Converter Control Unit Reactive Power (kVAr) Temperature Trafo-2 (°C) Reactive Power (kVAr) Temperature Shaft Bearing-1 (°C) Gearbox_Distributor_Temperature (°C) Moment D Filtered (kNm) Moment D Direction (kNm) N-set 1 (rpm) Operating State ( ) Power Factor ( ) Temperature Shaft Bearing-2 (°C) Temperature_Nacelle (°C) Voltage A-N (V) Temperature Axis Box-3 (°C) Voltage C-N (V) Temperature Axis Box-2 (°C) Temperature Axis Box-1 (°C) Voltage B-N (V) Nacelle Position_Degree (°) Converter Control Unit Voltage (V) Temperature Battery Box-3 (°C) Temperature Battery Box-2 (°C) Temperature Battery Box-1 (°C) Hydraulic Prepressure (bar) Angle Rotor Position (°) Temperature Tower Base (°C) Pitch Offset-2 Asymmetric Load Controller (°) Pitch Offset Tower Feedback (°) Line Frequency (Hz) Internal Power Limit (kW) Circuit Breaker cut-ins ( ) Particle Counter ( ) Tower Accelaration Normal Raw (mm/s²) Torque Offset Tower Feedback (Nm) External Power Limit (kW) Blade-2 Actual Value_Angle-B (°) Blade-1 Actual Value_Angle-B (°) Blade-3 Actual Value_Angle-B (°) Temperature Heat Exchanger Converter Control Unit (°C) Tower Accelaration Lateral Raw (mm/s²) Temperature Ambient (°C) Nacelle Revolution ( ) Pitch Offset-1 Asymmetric Load Controller (°) Tower Deflection (ms) Pitch Offset-3 Asymmetric Load Controller (°) Wind Deviation 1 seconds (°) Wind Deviation 10 seconds (°) Proxy Sensor_Degree-135 (mm) State and Fault ( ) Proxy Sensor_Degree-225 (mm) Blade-3 Actual Value_Angle-A (°) Scope CH 4 ( ) Blade-2 Actual Value_Angle-A (°) Blade-1 Actual Value_Angle-A (°) Blade-2 Set Value_Degree (°) Pitch Demand Baseline_Degree (°) Blade-1 Set Value_Degree (°) Blade-3 Set Value_Degree (°) Moment Q Direction (kNm) Moment Q Filltered (kNm) Proxy Sensor_Degree-45 (mm) Turbine State ( ) Proxy Sensor_Degree-315 (mm) Preprocessing #!/usr/bin/env python # coding: utf-8 import numpy as np import csv import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) import pandas as pd import pickle import time import os import pandas as pd unzip dataset import zipfile path_to_zip_file=\u0026#34;enerjisa-uretim-hackathon.zip\u0026#34; with zipfile.ZipFile(path_to_zip_file, \u0026#39;r\u0026#39;) as zip_ref: zip_ref.extractall(path_to_zip_file[:-4]) take dataset files as a lit in “csvs” def find_the_way(path,file_format): files_add = [] for r, d, f in os.walk(path): for file in f: if file_format in file: files_add.append(os.path.join(r, file)) return files_add path=path_to_zip_file[:-4] csvs=find_the_way(path,\u0026#39;.csv\u0026#39;) csvs [\u0026#39;enerjisa-uretim-hackathon\\\\features.csv\u0026#39;, \u0026#39;enerjisa-uretim-hackathon\\\\feature_units.csv\u0026#39;, \u0026#39;enerjisa-uretim-hackathon\\\\power.csv\u0026#39;, \u0026#39;enerjisa-uretim-hackathon\\\\sample_submission.csv\u0026#39;] replace nan and inf value with 0 features=pd.read_csv(csvs[0]) labels=pd.read_csv(csvs[2]) features.replace([np.inf, -np.inf], np.nan, inplace=True) features=features.fillna(0) create and add a new feature related with timeseries ay_ve_gun=[] for i in features[\u0026#34;Timestamp\u0026#34;]: month=int(i[5:7])*100 day=(int(i[8:10])//10+1) if day==4: day=3 ay_ve_gun.append(month+day) features[\u0026#34;ay_ve_gun\u0026#34;]=ay_ve_gun split labelled and unlabelled data train_size=len(labels) main=features[0:train_size] submission=features[train_size:] add labels to dataframe main[\u0026#34;Power(kW)\u0026#34;]=labels[\u0026#34;Power(kW)\u0026#34;] show unlabeled data which we will not use submission Timestamp Gearbox_T1_High_Speed_Shaft_Temperature Gearbox_T3_High_Speed_Shaft_Temperature …","date":1653609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653609600,"objectID":"86dbe591bfc2bfe27f9bb5f863868fa4","permalink":"https://example.com/project/prediction-of-wind-turbine-power-generation-from-real-time-scada-data-main/readme/","publishdate":"2022-05-27T00:00:00Z","relpermalink":"/project/prediction-of-wind-turbine-power-generation-from-real-time-scada-data-main/readme/","section":"project","summary":"Prediction of wind turbine power generation from real-time SCADA data","tags":["Machine Learning"],"title":"Prediction of wind turbine power generation from real-time SCADA data","type":"project"},{"authors":null,"categories":null,"content":" Our solution is the winner of the Huawei UK University Challenge Competition 2021. Members: Rabia Yasa Kostas Kahraman Kostas The team presenter: Kahraman Kostas\nYou can download the original versions of the questions given in the competition here: Task 1 \u0026amp; Task 2 TASK 1 QUESTION 1.1 WiFi Similarity Introduction To get you started we’ve put together a simple problem to introduce some key indoor positioning concepts. Consider the following environment: a user is travelling in open space in the presence of 3 WiFi emitters (we call the data created by this user a trajectory). Each emitter has a unique mac address. The user is equipped with a smartphone that will periodically scan the WiFi environment and record the RSSI of each detected mac (in dB).\nFor this model we have used a standard log-loss free-space propagation model for each of the emitters. This is a simplistic model that works well in free space, but breaks down in real indoor environments with walls and other obstacles which can bounce the signals around in a more complex manner. In general we do expect to see a steep drop in RSSI over distance as the fixed energy from the emitting antenna is spread over an increasing area as the wave propagates. In the diagram below each circle denotes a drop of 10dB.\nThe user walks North-East from point (0,0) and there phone makes three scans of the environment. The data recorded at each scan is shown below.\nscan 0 -\u0026gt; {\u0026#39;green\u0026#39;: -60, \u0026#39;blue\u0026#39;: -66, \u0026#39;red\u0026#39;: -67} scan 1 -\u0026gt; {\u0026#39;green\u0026#39;: -58, \u0026#39;blue\u0026#39;: -61, \u0026#39;red\u0026#39;: -60} scan 2 -\u0026gt; {\u0026#39;green\u0026#39;: -66, \u0026#39;blue\u0026#39;: -62, \u0026#39;red\u0026#39;: -59} The complex and locally unique properties of the WiFi environment make it very useful for indoor positioning systems. For example in the below image scan 1 measures data at roughly the centroid of the three emitters and there is no other place in this environment where one could take a reading that would register similar RSSI values. Given a set of scans or “fingerprints” from independent trajectories, we are interested in calculating how similar they are in WiFi space as this is an indication of how close they are in real space.\nYour first challenge is to write a function to calculate the Euclidean Distance and Manhattan Distance metrics between each of the scans in the sample trajectory that we introduced above. Using the data from a single trajectory is a good way to test the quality of a similarity metric as we can get fairly accurate estimates of the true distance using the data from the phone’s intertial measurement unit (IMU) which is used by a pedestrian dead reckoning (PDR) module.\ndef euclidean(fp1, fp2): raise NotImplementedError def manhattan(fp1, fp2): raise NotImplementedError # solution of the above functions from scipy.spatial import distance def euclidean(fp1, fp2): fp1=list(fp1.values()) fp2=list(fp2.values()) return distance.euclidean(fp1, fp2) def manhattan(fp1, fp2): fp1=list(fp1.values()) fp2=list(fp2.values()) return distance.cityblock(fp1, fp2) import json import numpy as np import matplotlib.pyplot as plt from metrics import eval_dist_metric with open(\u0026#34;intro_trajectory_1.json\u0026#34;) as f: traj = json.load(f) ## Pre-calculate the pair indexes we are interested in keys = [] for fp1 in traj[\u0026#39;fps\u0026#39;]: for fp2 in traj[\u0026#39;fps\u0026#39;]: # only calculate the upper triangle if fp1[\u0026#39;step_index\u0026#39;] \u0026gt; fp2[\u0026#39;step_index\u0026#39;]: keys.append((fp1[\u0026#39;step_index\u0026#39;], fp2[\u0026#39;step_index\u0026#39;])) ## Get the distances from PDR true_d = {} for step1 in traj[\u0026#39;steps\u0026#39;]: for step2 in traj[\u0026#39;steps\u0026#39;]: key = (step1[\u0026#39;step_index\u0026#39;],step2[\u0026#39;step_index\u0026#39;]) if key in keys: true_d[key] = abs(step1[\u0026#39;di\u0026#39;] - step2[\u0026#39;di\u0026#39;]) euc_d = {} man_d = {} for fp1 in traj[\u0026#39;fps\u0026#39;]: for fp2 in traj[\u0026#39;fps\u0026#39;]: key = (fp1[\u0026#39;step_index\u0026#39;],fp2[\u0026#39;step_index\u0026#39;]) if key in keys: euc_d[key] = euclidean(fp1[\u0026#39;profile\u0026#39;],fp2[\u0026#39;profile\u0026#39;]) man_d[key] = manhattan(fp1[\u0026#39;profile\u0026#39;],fp2[\u0026#39;profile\u0026#39;]) print(\u0026#34;Euclidean Average Error\u0026#34;) print(f\u0026#39;{eval_dist_metric(euc_d, true_d):.2f}\u0026#39;) print(\u0026#34;Manhattan Average Error\u0026#34;) print(f\u0026#39;{eval_dist_metric(man_d, true_d):.2f}\u0026#39;) Euclidean Average Error 9.29 Manhattan Average Error 4.90 If you correctly implemented the functions you should have seen that the average error for the euclidean metric was 9.29 whilst the Manhattan was only 4.90. So for this data, the Manhattan distance is a better estimate of the true distance.\nThis is of course a very simplistic model. Indeed, there is no direct relationship between the RSSI values and the free space distance in this way. Typically, when we create our own estimates for distance we would use the known pdr distances from within a trajectory to fit the numeric score to a physical distance estimate.\n1.2 WiFi Similarity Metric For your main challenge, we would like you to develop your own metric to estimate the real-world distance between two scans, based solely on their WiFi fingerprints. We will provide you with real crowdsourced data collected early in 2021 from a single mall. The data will contain 114661 fingerprints scans and 879824 distances between the scans. The distances will be …","date":1651017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651017600,"objectID":"42e440a0e3976a1f7ea38de24133ac38","permalink":"https://example.com/project/huawei-uk-university-challenge-competition-2021-main/readme/","publishdate":"2022-04-27T00:00:00Z","relpermalink":"/project/huawei-uk-university-challenge-competition-2021-main/readme/","section":"project","summary":"Our solution is the winner of the Huawei UK University Challenge Competition 2021","tags":["Deep Learning"],"title":"Data Science for Indoor positioning","type":"project"},{"authors":null,"categories":null,"content":"NSL KDD binary classification with Transformer I used it to classify the NSL-KDD dataset by making a slight change on the code I got from the keras documentation page.\nimporting of required libraries import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers import numpy as np import pandas as pd from sklearn import preprocessing from sklearn.model_selection import train_test_split Implement multi head self attention as a Keras layer class MultiHeadSelfAttention(layers.Layer): def __init__(self, embed_dim, num_heads=8): super(MultiHeadSelfAttention, self).__init__() self.embed_dim = embed_dim self.num_heads = num_heads if embed_dim % num_heads != 0: raise ValueError( f\u0026#34;embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\u0026#34; ) self.projection_dim = embed_dim // num_heads self.query_dense = layers.Dense(embed_dim) self.key_dense = layers.Dense(embed_dim) self.value_dense = layers.Dense(embed_dim) self.combine_heads = layers.Dense(embed_dim) def attention(self, query, key, value): score = tf.matmul(query, key, transpose_b=True) dim_key = tf.cast(tf.shape(key)[-1], tf.float32) scaled_score = score / tf.math.sqrt(dim_key) weights = tf.nn.softmax(scaled_score, axis=-1) output = tf.matmul(weights, value) return output, weights def separate_heads(self, x, batch_size): x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) return tf.transpose(x, perm=[0, 2, 1, 3]) def call(self, inputs): # x.shape = [batch_size, seq_len, embedding_dim] batch_size = tf.shape(inputs)[0] query = self.query_dense(inputs) # (batch_size, seq_len, embed_dim) key = self.key_dense(inputs) # (batch_size, seq_len, embed_dim) value = self.value_dense(inputs) # (batch_size, seq_len, embed_dim) query = self.separate_heads( query, batch_size ) # (batch_size, num_heads, seq_len, projection_dim) key = self.separate_heads( key, batch_size ) # (batch_size, num_heads, seq_len, projection_dim) value = self.separate_heads( value, batch_size ) # (batch_size, num_heads, seq_len, projection_dim) attention, weights = self.attention(query, key, value) attention = tf.transpose( attention, perm=[0, 2, 1, 3] ) # (batch_size, seq_len, num_heads, projection_dim) concat_attention = tf.reshape( attention, (batch_size, -1, self.embed_dim) ) # (batch_size, seq_len, embed_dim) output = self.combine_heads( concat_attention ) # (batch_size, seq_len, embed_dim) return output Implement a Transformer block as a layer class TransformerBlock(layers.Layer): def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): super(TransformerBlock, self).__init__() self.att = MultiHeadSelfAttention(embed_dim, num_heads) self.ffn = keras.Sequential( [layers.Dense(ff_dim, activation=\u0026#34;relu\u0026#34;), layers.Dense(embed_dim),] ) self.layernorm1 = layers.LayerNormalization(epsilon=1e-6) self.layernorm2 = layers.LayerNormalization(epsilon=1e-6) self.dropout1 = layers.Dropout(rate) self.dropout2 = layers.Dropout(rate) def call(self, inputs, training): attn_output = self.att(inputs) attn_output = self.dropout1(attn_output, training=training) out1 = self.layernorm1(inputs + attn_output) ffn_output = self.ffn(out1) ffn_output = self.dropout2(ffn_output, training=training) return self.layernorm2(out1 + ffn_output) Implement embedding layer Two seperate embedding layers, one for tokens, one for token index (positions).\nclass TokenAndPositionEmbedding(layers.Layer): def __init__(self, maxlen, vocab_size, embed_dim): super(TokenAndPositionEmbedding, self).__init__() self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim) self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) def call(self, x): maxlen = tf.shape(x)[-1] positions = tf.range(start=0, limit=maxlen, delta=1) positions = self.pos_emb(positions) x = self.token_emb(x) return x + positions prepare NSL KDD dataset reading CSV files # c_names ---\u0026gt; column names c_names = [\u0026#34;duration\u0026#34;,\u0026#34;protocol_type\u0026#34;,\u0026#34;service\u0026#34;,\u0026#34;flag\u0026#34;,\u0026#34;src_bytes\u0026#34;, \u0026#34;dst_bytes\u0026#34;,\u0026#34;land\u0026#34;,\u0026#34;wrong_fragment\u0026#34;,\u0026#34;urgent\u0026#34;,\u0026#34;hot\u0026#34;,\u0026#34;num_failed_logins\u0026#34;, \u0026#34;logged_in\u0026#34;,\u0026#34;num_compromised\u0026#34;,\u0026#34;root_shell\u0026#34;,\u0026#34;su_attempted\u0026#34;,\u0026#34;num_root\u0026#34;, \u0026#34;num_file_creations\u0026#34;,\u0026#34;num_shells\u0026#34;,\u0026#34;num_access_files\u0026#34;,\u0026#34;num_outbound_cmds\u0026#34;, \u0026#34;is_host_login\u0026#34;,\u0026#34;is_guest_login\u0026#34;,\u0026#34;count\u0026#34;,\u0026#34;srv_count\u0026#34;,\u0026#34;serror_rate\u0026#34;, \u0026#34;srv_serror_rate\u0026#34;,\u0026#34;rerror_rate\u0026#34;,\u0026#34;srv_rerror_rate\u0026#34;,\u0026#34;same_srv_rate\u0026#34;, \u0026#34;diff_srv_rate\u0026#34;,\u0026#34;srv_diff_host_rate\u0026#34;,\u0026#34;dst_host_count\u0026#34;,\u0026#34;dst_host_srv_count\u0026#34;, \u0026#34;dst_host_same_srv_rate\u0026#34;,\u0026#34;dst_host_diff_srv_rate\u0026#34;,\u0026#34;dst_host_same_src_port_rate\u0026#34;, \u0026#34;dst_host_srv_diff_host_rate\u0026#34;,\u0026#34;dst_host_serror_rate\u0026#34;,\u0026#34;dst_host_srv_serror_rate\u0026#34;, \u0026#34;dst_host_rerror_rate\u0026#34;,\u0026#34;dst_host_srv_rerror_rate\u0026#34;,\u0026#34;labels\u0026#34;,\u0026#34;difficulty_degree\u0026#34;] train = pd.read_csv( \u0026#34;data/KDDTrain+.csv\u0026#34;, names=c_names) # train file test = pd.read_csv(\u0026#34;data/KDDTest+.csv\u0026#34;, names=c_names) # test file deletion of unnecessary feature (difficulty_degree) del train[\u0026#34;difficulty_degree\u0026#34;] del test[\u0026#34;difficulty_degree\u0026#34;] Converting object features to categories first and then to dummy tables (except “labels”) for i in c_names: …","date":1651017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651017600,"objectID":"2c68a61a0cf8e714040703f1acb0b2a4","permalink":"https://example.com/project/nsl-kdd-binary-classification-with-transformer-master/readme/","publishdate":"2022-04-27T00:00:00Z","relpermalink":"/project/nsl-kdd-binary-classification-with-transformer-master/readme/","section":"project","summary":"NSL KDD binary classification with Transformer","tags":["Deep Learning"],"title":"NSL KDD binary classification with Transformer","type":"project"},{"authors":null,"categories":null,"content":"Practice Round - Hash Code 2022 One Pizza Problem You are opening a small pizzeria. In fact, your pizzeria is so small that you decided to offer only one type of pizza. Now you need to decide what ingredients to include (peppers? tomatoes? both?).\nEveryone has their own pizza preferences. Each of your potential clients has some ingredients they like, and maybe some ingredients they dislike. Each client will come to your pizzeria if both conditions are true:\nall the ingredients they like are on the pizza, and none of the ingredients they dislike are on the pizza Each client is OK with additional ingredients they neither like or dislike being present on the pizza. Your task is to choose which ingredients to put on your only pizza type, to maximize the number of clients that will visit your pizzeria.\nInput The first line contains one integer 1≤C≤10^5 the number of potential clients.\nThe following 2XC lines describe the clients’ preferences in the following format:\nFirst line contains integer 1≤L≤5, followed by L names of ingredients a client likes, delimited by spaces. Second line contains integer 0≤D≤5,, followed by D names of ingredients a client dislikes, delimited by spaces. Each ingredient name consists of between 1 and 15 ASCII characters. Each character is one of the lowercase letters (a-z) or a digit (0-9).\nSubmission The submission should consist of one line consisting of a single number 0≤N followed by a list of N ingredients to put on the only pizza available in the pizzeria, separated by spaces. The list of ingredients should contain only the ingredients mentioned by at least one client, without duplicates.\nScoring A solution scores one point for each client that will come to your pizzeria. A client will come to your pizzeria if all the ingredients they like are on the pizza and none of the ingredients they dislike are on the pizza.\nSample In the Sample Input there are 3 potential clients:\nThe first client likes 2 ingredients, cheese and peppers, and does not dislike anything. The second client likes only basil and dislikes only pineapple. The third client likes mushrooms and tomatoes and dislikes only basil The picture below shows the preferences of 3 potential clients. In this particular Sample Output, we choose to use 4 ingredients in the pizza: cheese, mushrooms, tomatoes, and peppers.\nThe first client likes the pizza because it contains both cheese and peppers, which they like. The second client does not like the pizza: it does not contain basil which they like. The third client likes the pizza because it contains mushrooms and tomatoes, which they like, and does not contain basil which they do not like. This means a submission of this output would score 2 points for this case, because two clients (the first and third ones) would like this pizza.\nSolutions import os folder creation function. creates the folders to put the outputs. def folder(f_name): try: if not os.path.exists(f_name): os.makedirs(f_name) except OSError: print (\u0026#34;Tthe folder could not be created!\u0026#34;) The function that finds the locations of required files def find_the_way(path,file_format): files_add = [] # r=root, d=directories, f = files for r, d, f in os.walk(path): for file in f: if file_format in file: files_add.append(os.path.join(r, file)) return files_add files_add=find_the_way(\u0026#39;./input/\u0026#39;,\u0026#39;.txt\u0026#39;) This Function evaluates the results obtained. as output, it gives you how many people can eat the pizza you created. def evaluate(outputfolder): total=0 flag=0 for myfile in files_add: like=[] dislike=[] counter=0 person=0 output_filename=outputfolder+myfile[8:] with open(output_filename, \u0026#34;r\u0026#34;) as file: output=file.readline() output=output.split(\u0026#34; \u0026#34;) output=output[1:] with open(myfile, \u0026#34;r\u0026#34;) as file: while True: line=file.readline() if line==\u0026#34;\u0026#34;:break if counter!=0: line=line.replace(\u0026#34;\\n\u0026#34;,\u0026#34;\u0026#34;) line=line.split(\u0026#34; \u0026#34;) if counter%2!=0: temp1=1 for i in line[1:]: if i not in output: temp1=0 if counter%2==0: temp2=1 for i in line[1:]: if i in output: temp2=0 person+=(temp1 and temp2) counter+=1 print(\u0026#39;%-15s %-30s %-10s %-10s\u0026#39; % (\u0026#34;File name: \u0026#34;,str(myfile),\u0026#34; score:\u0026#34;, str(person) )) total=total+person print(f\u0026#39;Total score: {total}\u0026#39;) Input File List files_add [\u0026#39;./input/a_an_example.in.txt\u0026#39;, \u0026#39;./input/b_basic.in.txt\u0026#39;, \u0026#39;./input/c_coarse.in.txt\u0026#39;, \u0026#39;./input/d_difficult.in.txt\u0026#39;, \u0026#39;./input/e_elaborate.in.txt\u0026#39;] VERSION 1 The first and simplest method that comes to mind is to bring together all the desired and unwanted materials in separate lists, and then remove the unwanted materials from the desired list.\nThis method has been followed in version 1.\noutputfolder=\u0026#34;outputV1/\u0026#34; folder(outputfolder) for myfile in files_add: like=[] dislike=[] counter=0 with open(myfile, \u0026#34;r\u0026#34;) as file: while True: line=file.readline() if line==\u0026#34;\u0026#34;:break if counter!=0: line=line.replace(\u0026#34;\\n\u0026#34;,\u0026#34;\u0026#34;) #print(line,counter) line=line.split(\u0026#34; \u0026#34;) if counter%2==0: for i in line[1:]: if i not in dislike: dislike.append(i) else: for i in line[1:]: if i not in like: like.append(i) counter+=1 …","date":1651017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651017600,"objectID":"0474713fa702c40287ce348e8db92493","permalink":"https://example.com/project/practice-round-hash-code-2022-one-pizza-main/readme/","publishdate":"2022-04-27T00:00:00Z","relpermalink":"/project/practice-round-hash-code-2022-one-pizza-main/readme/","section":"project","summary":"You are opening a small pizzeria. In fact, your pizzeria is so small that you decided to offer only one type of pizza. Now you need to decide what ingredients to include","tags":["Deep Learning"],"title":"Practice Round - Hash Code 2022","type":"project"},{"authors":null,"categories":null,"content":"World name statistics forenames and statistics by country Creating a dataset of forenames by country using Web Crawling. This code extracted the forenames and their gender (number of males and females carrying the name), number and frequency of use from the forebears.io website and saved them in a CSV file for each country. The view of the generated files is as follows (Country: Turkey) Name Male Female Incidence Frequency 0 Mehmet 100 0 2557203 1:30 1 Fatma 0 100 1808748 1:43 2 Mustafa 100 0 1461828 1:53 3 Ayşe 0 100 1413659 1:55 4 Ahmet 100 0 1254943 1:62 ... ... ... ... ... ... 995 Pembe 0 100 9488 1:8200 996 Sevdiye 0 100 9458 1:8226 997 Gürbüz 96 4 9448 1:8235 998 Amine 0 100 9438 1:8243 999 Şemse 0 100 9420 1:8259 1000 rows × 5 columns\n# required modules import re import codecs from urllib.request import Request, urlopen Alphabetical list of scanned countries countrys=[\u0026#34;afghanistan\u0026#34;, \u0026#34;albania\u0026#34;, \u0026#34;algeria\u0026#34;, \u0026#34;andorra\u0026#34;, \u0026#34;angola\u0026#34;, \u0026#34;antigua-and-barbuda\u0026#34;, \u0026#34;argentina\u0026#34;, \u0026#34;armenia\u0026#34;, \u0026#34;australia\u0026#34;, \u0026#34;austria\u0026#34;, \u0026#34;azerbaijan\u0026#34;, \u0026#34;bahamas\u0026#34;, \u0026#34;bahrain\u0026#34;, \u0026#34;bangladesh\u0026#34;, \u0026#34;barbados\u0026#34;, \u0026#34;belarus\u0026#34;, \u0026#34;belgium\u0026#34;, \u0026#34;belize\u0026#34;, \u0026#34;benin\u0026#34;, \u0026#34;bhutan\u0026#34;, \u0026#34;bolivia\u0026#34;, \u0026#34;bosnia-and-herzegovina\u0026#34;, \u0026#34;botswana\u0026#34;, \u0026#34;brazil\u0026#34;, \u0026#34;brunei\u0026#34;, \u0026#34;bulgaria\u0026#34;, \u0026#34;burkina-faso\u0026#34;, \u0026#34;burundi\u0026#34;, #\u0026#34;côte-d\u0026#39;ivoire\u0026#34;, \u0026#34;cabo-verde\u0026#34;, \u0026#34;cambodia\u0026#34;, \u0026#34;cameroon\u0026#34;, \u0026#34;canada\u0026#34;, \u0026#34;central-african-republic\u0026#34;, \u0026#34;chad\u0026#34;, \u0026#34;chile\u0026#34;, \u0026#34;china\u0026#34;, \u0026#34;colombia\u0026#34;, \u0026#34;comoros\u0026#34;, \u0026#34;congo\u0026#34;,\u0026#34;congo-brazzaville\u0026#34;, \u0026#34;costa-rica\u0026#34;, \u0026#34;croatia\u0026#34;, \u0026#34;cuba\u0026#34;, \u0026#34;cyprus\u0026#34;, \u0026#34;czechia\u0026#34;,\u0026#34;czech-republic\u0026#34;, \u0026#34;democratic-republic-of-the-congo\u0026#34;, \u0026#34;denmark\u0026#34;, \u0026#34;djibouti\u0026#34;, \u0026#34;dominica\u0026#34;, \u0026#34;dominican-republic\u0026#34;, \u0026#34;ecuador\u0026#34;, \u0026#34;egypt\u0026#34;, \u0026#34;el-salvador\u0026#34;, \u0026#34;equatorial-guinea\u0026#34;, \u0026#34;eritrea\u0026#34;, \u0026#34;estonia\u0026#34;, \u0026#34;eswatini\u0026#34;,\u0026#34;swaziland\u0026#34;, \u0026#34;ethiopia\u0026#34;, \u0026#34;fiji\u0026#34;, \u0026#34;finland\u0026#34;, \u0026#34;france\u0026#34;, \u0026#34;gabon\u0026#34;, \u0026#34;gambia\u0026#34;, \u0026#34;georgia\u0026#34;, \u0026#34;germany\u0026#34;, \u0026#34;ghana\u0026#34;, \u0026#34;greece\u0026#34;, \u0026#34;grenada\u0026#34;, \u0026#34;guatemala\u0026#34;, \u0026#34;guinea\u0026#34;, \u0026#34;guinea-bissau\u0026#34;, \u0026#34;guyana\u0026#34;, \u0026#34;haiti\u0026#34;, \u0026#34;holy-see\u0026#34;, \u0026#34;honduras\u0026#34;, \u0026#34;hungary\u0026#34;, \u0026#34;iceland\u0026#34;, \u0026#34;india\u0026#34;, \u0026#34;indonesia\u0026#34;, \u0026#34;iran\u0026#34;, \u0026#34;iraq\u0026#34;, \u0026#34;ireland\u0026#34;, \u0026#34;israel\u0026#34;, \u0026#34;italy\u0026#34;, \u0026#34;jamaica\u0026#34;, \u0026#34;japan\u0026#34;, \u0026#34;jordan\u0026#34;, \u0026#34;kazakhstan\u0026#34;, \u0026#34;kenya\u0026#34;, \u0026#34;kiribati\u0026#34;, \u0026#34;kuwait\u0026#34;, \u0026#34;kyrgyzstan\u0026#34;, \u0026#34;laos\u0026#34;, \u0026#34;latvia\u0026#34;, \u0026#34;lebanon\u0026#34;, \u0026#34;lesotho\u0026#34;, \u0026#34;liberia\u0026#34;, \u0026#34;libya\u0026#34;, \u0026#34;liechtenstein\u0026#34;, \u0026#34;lithuania\u0026#34;, \u0026#34;luxembourg\u0026#34;, \u0026#34;madagascar\u0026#34;, \u0026#34;malawi\u0026#34;, \u0026#34;malaysia\u0026#34;, \u0026#34;maldives\u0026#34;, \u0026#34;mali\u0026#34;, \u0026#34;malta\u0026#34;, \u0026#34;marshall-islands\u0026#34;, \u0026#34;mauritania\u0026#34;, \u0026#34;mauritius\u0026#34;, \u0026#34;mexico\u0026#34;, \u0026#34;micronesia\u0026#34;, \u0026#34;moldova\u0026#34;, \u0026#34;monaco\u0026#34;, \u0026#34;mongolia\u0026#34;, \u0026#34;montenegro\u0026#34;, \u0026#34;morocco\u0026#34;, \u0026#34;mozambique\u0026#34;, \u0026#34;myanmar-(formerly-burma)\u0026#34;, \u0026#34;namibia\u0026#34;, \u0026#34;nauru\u0026#34;, \u0026#34;nepal\u0026#34;, \u0026#34;netherlands\u0026#34;, \u0026#34;new-zealand\u0026#34;, \u0026#34;nicaragua\u0026#34;, \u0026#34;niger\u0026#34;, \u0026#34;nigeria\u0026#34;, \u0026#34;north-korea\u0026#34;, \u0026#34;north-macedonia\u0026#34;, \u0026#34;norway\u0026#34;, \u0026#34;oman\u0026#34;, \u0026#34;pakistan\u0026#34;, \u0026#34;palau\u0026#34;, \u0026#34;palestine-state\u0026#34;, \u0026#34;panama\u0026#34;, \u0026#34;papua-new-guinea\u0026#34;, \u0026#34;paraguay\u0026#34;, \u0026#34;peru\u0026#34;, \u0026#34;philippines\u0026#34;, \u0026#34;poland\u0026#34;, \u0026#34;portugal\u0026#34;, \u0026#34;qatar\u0026#34;, \u0026#34;romania\u0026#34;, \u0026#34;russia\u0026#34;, \u0026#34;rwanda\u0026#34;, \u0026#34;saint-kitts-and-nevis\u0026#34;, \u0026#34;saint-lucia\u0026#34;, \u0026#34;saint-vincent-and-the-grenadines\u0026#34;, \u0026#34;samoa\u0026#34;, \u0026#34;san-marino\u0026#34;, \u0026#34;sao-tome-and-principe\u0026#34;, \u0026#34;saudi-arabia\u0026#34;, \u0026#34;senegal\u0026#34;, \u0026#34;serbia\u0026#34;, \u0026#34;seychelles\u0026#34;, \u0026#34;sierra-leone\u0026#34;, \u0026#34;singapore\u0026#34;, \u0026#34;slovakia\u0026#34;, \u0026#34;slovenia\u0026#34;, \u0026#34;solomon-islands\u0026#34;, \u0026#34;somalia\u0026#34;, \u0026#34;south-africa\u0026#34;, \u0026#34;south-korea\u0026#34;, \u0026#34;south-sudan\u0026#34;, \u0026#34;spain\u0026#34;, \u0026#34;sri-lanka\u0026#34;, \u0026#34;sudan\u0026#34;, \u0026#34;suriname\u0026#34;, \u0026#34;sweden\u0026#34;, \u0026#34;switzerland\u0026#34;, \u0026#34;syria\u0026#34;, \u0026#34;tajikistan\u0026#34;, \u0026#34;tanzania\u0026#34;, \u0026#34;thailand\u0026#34;, \u0026#34;timor-leste\u0026#34;, \u0026#34;togo\u0026#34;, \u0026#34;tonga\u0026#34;, \u0026#34;trinidad-and-tobago\u0026#34;, \u0026#34;tunisia\u0026#34;, \u0026#34;turkey\u0026#34;, \u0026#34;turkmenistan\u0026#34;, \u0026#34;tuvalu\u0026#34;, \u0026#34;uganda\u0026#34;, \u0026#34;ukraine\u0026#34;, \u0026#34;united-arab-emirates\u0026#34;, \u0026#34;united-kingdom\u0026#34;, \u0026#34;united-states-of-america\u0026#34;, \u0026#34;uruguay\u0026#34;, \u0026#34;uzbekistan\u0026#34;, \u0026#34;vanuatu\u0026#34;, \u0026#34;venezuela\u0026#34;, \u0026#34;vietnam\u0026#34;, \u0026#34;yemen\u0026#34;, \u0026#34;zambia\u0026#34;, \u0026#34;zimbabwe\u0026#34;] header=\u0026#34;Name,Male,Female,Incidence,Frequency\\n\u0026#34; for country in countrys: try: site=\u0026#34;https://forebears.io/\u0026#34;+country+\u0026#34;/forenames\u0026#34; req = Request(site, headers={\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0\u0026#39;}) webpage = urlopen(req,timeout=10).read() s=webpage.decode(\u0026#34;utf-8\u0026#34;) a=\u0026#34;Frequency\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt;\u0026lt;/thead\u0026gt;\u0026lt;tbody\u0026gt;\u0026#34; z=\u0026#34;\u0026lt;/tbody\u0026gt;\u0026#34; start=s.find(a)+len(a) stop=s.find(z) s1=s[start:stop] s1=s1.split(\u0026#34;\u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;\u0026#34;) cn=country+\u0026#34;.csv\u0026#34; ths = codecs.open(cn, \u0026#34;w\u0026#34;, \u0026#34;utf-8\u0026#34;) ths.write(header) for i in s1[1:]: #Frequency result = re.search(\u0026#39;%\u0026#34;\u0026gt;(.*)\u0026lt;/span\u0026gt;\u0026#39;, i) Frequency=result.group(1) Frequency=Frequency.replace(\u0026#34;,\u0026#34;,\u0026#34;\u0026#34;) #Forename result = re.search(\u0026#39;forenames/(.*)\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026#39;, i) result=result.group(1) result=result.split(\u0026#34;\u0026gt;\u0026#34;) Forename=result[1] #Incidence result = re.search(\u0026#39;\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;(.*)\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;span\u0026#39;, i) Incidence=result.group(1) Incidence=Incidence.replace(\u0026#34;,\u0026#34;,\u0026#34;\u0026#34;) #gender and ratio try: if \u0026#34;\u0026gt;100%\u0026lt;\u0026#34; in i: result = re.search(\u0026#39;class=\u0026#34;(.*)\u0026#34; style=\u0026#39;, i) result=result.group(1) #print(result) if result==\u0026#34;f\u0026#34;: female=100 male=0 else: female=0 male=100 else: result = re.search(\u0026#39;\u0026#34;width:(.*)px;\u0026#39;, i) result=result.group(1) key1=\u0026#34;px\u0026#34; key2=\u0026#34;width\u0026#34; start=result.find(key1) stop=result.find(key2)+len(key2)+1 male=result[:start] female=result[stop:] except: male=0 female=0 line=[Forename,male,female,Incidence,Frequency] line=str(line).replace(\u0026#34;[\u0026#34;,\u0026#34;\u0026#34;) line=str(line).replace(\u0026#34;]\u0026#34;,\u0026#34;\u0026#34;) line=str(line).replace(\u0026#34;, \u0026#34;,\u0026#34;,\u0026#34;) line=str(line).replace(\u0026#34;\\\u0026#39;\u0026#34;,\u0026#34;\u0026#34;) ths.write(str(line)+\u0026#34;\\n\u0026#34;) ths.close() print(\u0026#34;OK :\u0026#34;, country) except: print(\u0026#34;Error:\u0026#34;, country ) OK : afghanistan OK : albania OK : algeria OK : andorra OK : angola OK : …","date":1651017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651017600,"objectID":"cfb4dab7887b0dfa3171ab30d9407d9c","permalink":"https://example.com/project/name-statistics-by-country-in-the-world-main/readme/","publishdate":"2022-04-27T00:00:00Z","relpermalink":"/project/name-statistics-by-country-in-the-world-main/readme/","section":"project","summary":"forenames and statistics by country","tags":["Deep Learning"],"title":"World name statistics","type":"project"},{"authors":null,"categories":null,"content":"Turkish Students win Huawei UK University Challenge Competition 2021. TWO Turkish students doing a PhD in computer science in the UK won the Huawei UK University Challenge Competition 2021.\nKahraman Koştaş studying at the Heriot Watt University along with Rabia Yaşa Koştaş who is enrolled at The University of Edinburgh, are in the UK part of the scholarship program organised by the Republic of Turkey to study abroad for postgraduate studies.\nThe PhD students won the top prize in the annual event organised by Huawei UK Research Centre. The group were given two tasks to find a solutions looking at artificial intelligence, with fingerprint and WIFI technology. Their solution won out of hundred of applications, earning them the top prize of £7,000 as well as Huawei equipment and medals.\nSpeaking to Londra Gazete they said “We came first in the Huawei UK University Challenge Competition 2021, organized by Huawei company across the UK. Our team’s name was Turquoise. Many UK universities participated in this competition at all levels (275 students and 150+ teams).\n“As a result of this competition, our group was awarded the first place with a gold medal, certificate and financial awards (2 matepad pro tablet computer and £7000).\n“The concept of the competition was the solution of indoor positioning problems using data science (Data Science for Indoor positioning). For those who are unfamiliar with the subject, a more comprehensive and simple recipe can be called “coding competition with artificial intelligence”. Such competitions are very important in terms of enabling the theoretical knowledge taught at the university to be applied to real life problems and bringing academia and industry together.”\n","date":1643241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643241600,"objectID":"34dcf26e3a900f6353495c21e1fd1293","permalink":"https://example.com/post/2022-01-26-huawei/","publishdate":"2022-01-27T00:00:00Z","relpermalink":"/post/2022-01-26-huawei/","section":"post","summary":"Turkish Students win Huawei UK University Challenge Competition 2021. TWO Turkish students doing a PhD in computer science in the UK won the Huawei UK University Challenge Competition 2021.\nKahraman Koştaş studying at the Heriot Watt University along with Rabia Yaşa Koştaş who is enrolled at The University of Edinburgh, are in the UK part of the scholarship program organised by the Republic of Turkey to study abroad for postgraduate studies.","tags":["Huawei","University","Challenge"],"title":"Huawei UK University Challenge Competition 2021.","type":"post"},{"authors":null,"categories":null,"content":"Ebced ile Akıl Fikir Hesaplama Eskiden çocuklara isim seçmede izlenen yollardan bir tanesi de ebced ile ismin akıl ve fikir değerlerini hesaplayarak, en iyi-yüksek değeri vermekti. Bu bağlamda, seçtiğiniz isimlerin akıl ve fikir değerlerini hesaplayan basit bir program yazdım. Bu tarz yöntemleri muteber bulmuyorum ancak kafanızda çok fazla potansiyel isim varsa elemenize faydası dokunacaktır. Ayrıca kodlaması eğlenceliydi.\nAKIL: 1 ile 9 arasında bir değer alır. En yuksek 9 dur. Anne ve çocuk isimlerin ebced değerine göre hesaplanır.\nFİKİR: 1 ile 9 arasında bir değer alır. En yuksek 9 dur. Baba ve çocuk isimlerin ebced değerine göre hesaplanır.\nEşlerin İsim Uyumu: 1 ile 9 arasında bir değer alır. En yuksek 9 dur. Çocuğa verilebilecek en ez ve en fazla akıl-fikir değerini etkiler.\nBurç ve Yıldız: 1 ile 12 arasinda bir deger alır.ı Anne ve cocuk isimlerin ebced değerine göre hesaplanır. Burada yer alan burçlar, astronomi tabanlı burçlar değildir.\nKalan Burç Yıldız 1. KOÇ Merih 2. BOĞA Zühre 3. İKİZLER Utarid 4. YENGEÇ Ay 5. ASLAN Güneş 6. BAŞAK Utarid 7. TERAZİ Zühre 8. AKREP Merih 9. YAY Müşteri 10. OĞLAK Zühal 11. KOVA Zühal 12. BALIK Müşteri isimleri ve ebced değerlerini isimler ve ebced değerleri çalışmasında oluşturduğumuz küçük verisetinden alır. Bu yüzden eksik/yanlış isimler olabilir. Kontrol etmekten çekinmeyin.\n","date":1616803200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616803200,"objectID":"f6afa0f4fd083d3b20648a4f34bbdc8a","permalink":"https://example.com/project/ebced-akil-fikir-hesabi-main/readme/","publishdate":"2021-03-27T00:00:00Z","relpermalink":"/project/ebced-akil-fikir-hesabi-main/readme/","section":"project","summary":"Eskiden çocuklara isim seçmede izlenen yollardan bir tanesi de [ebced](https://tr.wikipedia.org/wiki/Ebced_hesab%C4%B1) ile ismin akıl ve fikir değerlerini hesaplayarak, en iyi-yüksek değeri vermekti. Bu bağlamda, seçtiğiniz isimlerin akıl ve fikir değerlerini hesaplayan basit bir program yazdım.","tags":["Deep Learning"],"title":"Ebced ile Akıl Fikir Hesaplama","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Anomaly-Detection-in-Networks-Using-Machine-Learning A thesis submitted for the degree of Master of Science in Computer Networks and Security\nThis file gives information on how to use the implementation files of “Anomaly Detection in Networks Using Machine Learning” ( A thesis submitted for the degree of Master of Science in Computer Networks and Security written by Kahraman Kostas ) Python 3.6 was used to create the application files. Before running the files, it must be ensured that Python 3.6 and the following libraries are installed.\nLibrary Task Sklearn Machine Learning Library Numpy Mathematical Operations Pandas Data Analysis Tools Matplotlib Graphics and Visuality The implementation phase consists of 5 steps, which are: 1-\tPre-processing 2-\tStatistics 3-\tAttack Filtering 4-\tFeature Selection 5-\tMachine Learning Implementation\nEach of these steps contains one or more Python files. The same file was saved with both “py” and “ipynb” extensions. The code they contain is exactly the same. The file with the ipynb extension has the advantage of saving the state of the last run of that file and the screen output.\nThus, screen output can be seen without re-running the files. Files with the ipynb extension can be run using the jupyter notebook program. When running the codes, the sequence numbers in the filenames should be followed.\nBecause the output of almost every program is the prerequisite for the operation of the next program. Each step is described in detail below.\n1 - Pre-processing This step consists of a single file (preprocessing.ipynb). For this program to work, the dataset (CIC-IDS2017) files must be in the “CSVs” folder in the same location as the program. The dataset files can be access here . (The reason that these files are given an external link is that the maximum limit of the file in the cseegit system is 10 MB)\nAs a result of executing this file, a file named “all_data.csv” is created. This file is a prerequisite for the other steps to work.\nThe most recent runtime of this file was recorded as 328 seconds. The technical specifications of the computer on which it is run are given below.\nCentral Processing Unit : Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz 2.90 GHz Random Access Memory : 8 GB (7.74 GB usable) Operating System : Windows 10 Pro 64-bit Graphics Processing Unit : AMD Readon (TM) 530 2 - Statistics This step consists of a single file (statistics.ipynb). This program examines the file “all_data.csv” and prints the statistics of attack and benign registry on this screen. It is not a prerequisite for any file. It only gives information.\nThe last run time of this file was recorded as 13 seconds.\n3 - Attack Filtering This step consists of a single file (attack_filter.ipynb). This program uses the “all_data.csv” file to create attack files and then it saves them in the “./attacks/” location. The Dataset contains 12 attack types in total. Therefore, 12 CSV files are created for these attacks. Within each file are 30% attack and 70% benign registry.This step is the prerequisite for the fourth and fifth steps. The last run time of this file was recorded as 304 seconds.\n4 - Feature Selection This step consists of two files.\na - feature_selection_for_attack_files.ipynb This program uses attack files located under the “attacks” folder. The aim of this program is to determine which features are important for each attack. For this purpose, It is used the Random Forest Regressor algorithm to calculate the importance weights of the features in the dataset. These acquired features are used in machine learning section As a screen output, it sorts its features and weights from large to small and shows them on the bar chart (average 20 attributes per attack type).\nThe most recent run of this file was recorded as 4817 seconds.\nb - feature_selection_for_all_data.ipynb This program applies the previous step to the entire data set. Thus, it creates the feature importance weights of that is valid for the entire dataset. It uses the “all_data.csv” file and the Random Forest Regressor algorithm. As a screen output, it sorts its features and weights from large to small and shows them on the bar chart (20 attributes in total for all attacks).\nThe last run time of this file was recorded as 25929 seconds.\n5 - Machine Learning Implementation This step applies the machine learning algorithms to the data set and consists of 5 files.\na - machine_learning_implementation_for_attack_files.ipynb this program uses the attack files under the “./attacks/” folder as a dataset. The features used are the 4 features with the highest weight for each file, produced by the feature_selection_for_attack_files file. This file applies 7 machine learning algorithms to each file 10 times and prints the results of these operations on the screen and in the file “./attacks/results_1.csv”. It also creates box and whisker graphics of the results and prints them both on the screen and in the “./attacks/result_graph_1/” folder.\nThe last run …","date":1535328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535328000,"objectID":"657a5065fe7605c7c6d33f97381cfdf7","permalink":"https://example.com/project/anomaly-detection-in-networks-using-machine-learning-master/readme/","publishdate":"2018-08-27T00:00:00Z","relpermalink":"/project/anomaly-detection-in-networks-using-machine-learning-master/readme/","section":"project","summary":"A thesis submitted for the degree of Master of Science in Computer Networks and Security","tags":["Machine Learning","Network Security"],"title":"Anomaly-Detection-in-Networks-Using-Machine-Learning","type":"project"}]